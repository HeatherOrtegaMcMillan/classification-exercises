{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2\n",
    "Given the following confusion matrix, evaluate (by hand) the model's performance.\n",
    "\n",
    "\n",
    "|               | pred dog   | pred cat   |\n",
    "|:------------  |-----------:|-----------:|\n",
    "| actual dog    |         46 |         7  |\n",
    "| actual cat    |         13 |         34 |\n",
    "\n",
    "In the context of this problem, what is a false positive?\n",
    "In the context of this problem, what is a false negative?\n",
    "How would you describe this model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- True PAWsitive: predict dog is dog\n",
    "- True Negative: predicts cat is cat\n",
    "- False PAWsitive: predicts dog is cat (boo!)\n",
    "- False Negative: predicts cat is dog\n",
    "\n",
    "----- \n",
    "- Accuracy = TP + TN / ALL RESULTS\n",
    "- A = (46 + 34) / (46 + 13 + 7 + 34)\n",
    "- A = 80/ 100 = .80 = 80% accuracy\n",
    "\n",
    "\n",
    "- recall = TP / (TP+FN)\n",
    "- R = 46 / (46 + 7)\n",
    "- 86.79%\n",
    "\n",
    "\n",
    "- precision = TP / (TP+FP)\n",
    "- P = 46 / (46 + 13)\n",
    "- 77.97%\n",
    "\n",
    "- Specificity = TN / (TN+FP)\n",
    "- S = 34 / 34 + 13\n",
    "- S = 34 / 47 = 72.34%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model has a good accuracy rate. The model's recall is particularly high. And that means that it will be very good guessing dogs.\n",
    "Website for dog adoption and cat adoption. You want it to guess the dogs right so that you don't have to manually go through and send the cats to the other website. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3\n",
    "You are working as a datascientist working for Codeup Cody Creator (C3 for short), a rubber-duck manufacturing plant.\n",
    "\n",
    "Unfortunately, some of the rubber ducks that are produced will have defects. Your team has built several models that try to predict those defects, and the data from their predictions can be found here.\n",
    "Use the predictions dataset and pandas to help answer the following questions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfc = pd.read_csv('c3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 4)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>actual</th>\n",
       "      <th>model1</th>\n",
       "      <th>model2</th>\n",
       "      <th>model3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>No Defect</td>\n",
       "      <td>No Defect</td>\n",
       "      <td>Defect</td>\n",
       "      <td>No Defect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>No Defect</td>\n",
       "      <td>No Defect</td>\n",
       "      <td>Defect</td>\n",
       "      <td>Defect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>No Defect</td>\n",
       "      <td>No Defect</td>\n",
       "      <td>Defect</td>\n",
       "      <td>No Defect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>No Defect</td>\n",
       "      <td>Defect</td>\n",
       "      <td>Defect</td>\n",
       "      <td>Defect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>No Defect</td>\n",
       "      <td>No Defect</td>\n",
       "      <td>Defect</td>\n",
       "      <td>No Defect</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      actual     model1  model2     model3\n",
       "0  No Defect  No Defect  Defect  No Defect\n",
       "1  No Defect  No Defect  Defect     Defect\n",
       "2  No Defect  No Defect  Defect  No Defect\n",
       "3  No Defect     Defect  Defect     Defect\n",
       "4  No Defect  No Defect  Defect  No Defect"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfc.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) An internal team wants to investigate the cause of the manufacturing defects. They tell you that they want to identify as many of the ducks that have a defect as possible. Which evaluation metric would be appropriate here? Which model would be the best fit for this use case?\n",
    "- We want to catch as many defect ducks as possible\n",
    "- specificity, aka recall for the negative class\n",
    "- recall where defects are the positive\n",
    "- because we care about the actual results (the actual real life defects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>actual</th>\n",
       "      <th>model1</th>\n",
       "      <th>model2</th>\n",
       "      <th>model3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Defect</td>\n",
       "      <td>No Defect</td>\n",
       "      <td>Defect</td>\n",
       "      <td>Defect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Defect</td>\n",
       "      <td>Defect</td>\n",
       "      <td>No Defect</td>\n",
       "      <td>Defect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>Defect</td>\n",
       "      <td>Defect</td>\n",
       "      <td>Defect</td>\n",
       "      <td>Defect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>Defect</td>\n",
       "      <td>Defect</td>\n",
       "      <td>Defect</td>\n",
       "      <td>Defect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>Defect</td>\n",
       "      <td>No Defect</td>\n",
       "      <td>No Defect</td>\n",
       "      <td>Defect</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    actual     model1     model2  model3\n",
       "13  Defect  No Defect     Defect  Defect\n",
       "30  Defect     Defect  No Defect  Defect\n",
       "65  Defect     Defect     Defect  Defect\n",
       "70  Defect     Defect     Defect  Defect\n",
       "74  Defect  No Defect  No Defect  Defect"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get subset of all actuals that are Defective \n",
    "# REAL LIFE \n",
    "subset = dfc[dfc.actual == 'Defect']\n",
    "subset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13     False\n",
       "30      True\n",
       "65      True\n",
       "70      True\n",
       "74     False\n",
       "87     False\n",
       "118    False\n",
       "135     True\n",
       "140    False\n",
       "147     True\n",
       "163     True\n",
       "171    False\n",
       "176    False\n",
       "186    False\n",
       "194     True\n",
       "196     True\n",
       "dtype: bool"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TRUE POSITIVES: Actual = Defect & Prediciton = Defect\n",
    "# FALSE NEGATIVES: Actuals = defect & prediction = not defected\n",
    "# positive = 'Defect'\n",
    "\n",
    "recall = subset.actual == subset.model1\n",
    "recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5625"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall2 = (subset.actual == subset.model2).mean()\n",
    "recall2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8125"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall3 = (subset.actual == subset.model3).mean()\n",
    "recall3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall for model 1 is 50.0%\n",
      "Recall for model 2 is 56.25%\n",
      "Recall for model 3 is 81.25%\n"
     ]
    }
   ],
   "source": [
    "#Rework into one nice thing\n",
    "\n",
    "# positives in this case are defects\n",
    "positive = 'Defect'\n",
    "\n",
    "# get the subset of what is reality. all the defect ducks in real life\n",
    "reality = dfc[dfc.actual == positive]\n",
    "\n",
    "# get recall for each model. compare models to reality and get the mean (from the boolean values)\n",
    "recall1 = (reality.actual == reality.model1).mean()\n",
    "recall2 = (reality.actual == reality.model2).mean()\n",
    "recall3 = (reality.actual == reality.model3).mean()\n",
    "\n",
    "# print it out\n",
    "print(f'Recall for model 1 is {recall1 * 100}%')\n",
    "print(f'Recall for model 2 is {recall2 * 100}%')\n",
    "print(f'Recall for model 3 is {recall3 * 100}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 3 would work the best here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Recently several stories in the local news have come out highlighting customers who received a rubber duck with a defect, and portraying C3 in a bad light. The PR team has decided to launch a program that gives customers with a defective duck a vacation to Hawaii. They need you to predict which ducks will have defects, but tell you the really don't want to accidentally give out a vacation package when the duck really doesn't have a defect. Which evaluation metric would be appropriate here? Which model would be the best fit for this use case?\n",
    "- In this case we care about the prediciton of the positive (the prediciton of a defective duck)\n",
    "- Prediction is what we care about TP / TP + FP\n",
    "- The false positives are expensive! \n",
    "- No free vacations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision rate for model 1 is 80.00%\n",
      "Precision rate for model 2 is 10.0%\n",
      "Precision rate for model 3 is 13.13%\n"
     ]
    }
   ],
   "source": [
    "# set positive as defect\n",
    "positive = 'Defect'\n",
    "\n",
    "# create subset of all times the model predicts positive\n",
    "# do for each model\n",
    "subset1 = dfc[dfc.model1 == positive]\n",
    "subset2 = dfc[dfc.model2 == positive]\n",
    "subset3 = dfc[dfc.model3 == positive]\n",
    "\n",
    "# compare the predicitons to the actuals \n",
    "precision1 = (subset1.actual == subset1.model1).mean()\n",
    "precision2 = (subset2.actual == subset2.model2).mean()\n",
    "precision3 = (subset3.actual == subset3.model3).mean()\n",
    "\n",
    "\n",
    "print(f'Precision rate for model 1 is {precision1:.2%}')\n",
    "print(f'Precision rate for model 2 is {precision2:.1%}')\n",
    "print(f'Precision rate for model 3 is {precision3:.2%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 1 is the best here as it will reduce the number of False positives. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4\n",
    "You are working as a data scientist for Gives You Paws ™, a subscription based service that shows you cute pictures of dogs or cats (or both for an additional fee).\n",
    "\n",
    "At Gives You Paws, anyone can upload pictures of their cats or dogs. The photos are then put through a two step process. First an automated algorithm tags pictures as either a cat or a dog (Phase I). Next, the photos that have been initially identified are put through another round of review, possibly with some human oversight, before being presented to the users (Phase II).\n",
    "\n",
    "Several models have already been developed with the data, and you can find their results here.\n",
    "\n",
    "Given this dataset, use pandas to create a baseline model (i.e. a model that just predicts the most common class) and answer the following questions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfg = pd.read_csv('gives_you_paws.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>actual</th>\n",
       "      <th>model1</th>\n",
       "      <th>model2</th>\n",
       "      <th>model3</th>\n",
       "      <th>model4</th>\n",
       "      <th>baseline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cat</td>\n",
       "      <td>cat</td>\n",
       "      <td>dog</td>\n",
       "      <td>cat</td>\n",
       "      <td>dog</td>\n",
       "      <td>dog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dog</td>\n",
       "      <td>dog</td>\n",
       "      <td>cat</td>\n",
       "      <td>cat</td>\n",
       "      <td>dog</td>\n",
       "      <td>dog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dog</td>\n",
       "      <td>cat</td>\n",
       "      <td>cat</td>\n",
       "      <td>cat</td>\n",
       "      <td>dog</td>\n",
       "      <td>dog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dog</td>\n",
       "      <td>dog</td>\n",
       "      <td>dog</td>\n",
       "      <td>cat</td>\n",
       "      <td>dog</td>\n",
       "      <td>dog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cat</td>\n",
       "      <td>cat</td>\n",
       "      <td>cat</td>\n",
       "      <td>dog</td>\n",
       "      <td>dog</td>\n",
       "      <td>dog</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  actual model1 model2 model3 model4 baseline\n",
       "0    cat    cat    dog    cat    dog      dog\n",
       "1    dog    dog    cat    cat    dog      dog\n",
       "2    dog    cat    cat    cat    dog      dog\n",
       "3    dog    dog    dog    cat    dog      dog\n",
       "4    cat    cat    cat    dog    dog      dog"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfg.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create baseline with the most common element in the actual category (which is dog)\n",
    "# I know there's a method for this but I didn't want to go re look for it\n",
    "dfg['baseline'] = dfg.actual.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>actual</th>\n",
       "      <th>model1</th>\n",
       "      <th>model2</th>\n",
       "      <th>model3</th>\n",
       "      <th>model4</th>\n",
       "      <th>baseline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cat</td>\n",
       "      <td>cat</td>\n",
       "      <td>dog</td>\n",
       "      <td>cat</td>\n",
       "      <td>dog</td>\n",
       "      <td>dog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dog</td>\n",
       "      <td>dog</td>\n",
       "      <td>cat</td>\n",
       "      <td>cat</td>\n",
       "      <td>dog</td>\n",
       "      <td>dog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dog</td>\n",
       "      <td>cat</td>\n",
       "      <td>cat</td>\n",
       "      <td>cat</td>\n",
       "      <td>dog</td>\n",
       "      <td>dog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dog</td>\n",
       "      <td>dog</td>\n",
       "      <td>dog</td>\n",
       "      <td>cat</td>\n",
       "      <td>dog</td>\n",
       "      <td>dog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cat</td>\n",
       "      <td>cat</td>\n",
       "      <td>cat</td>\n",
       "      <td>dog</td>\n",
       "      <td>dog</td>\n",
       "      <td>dog</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  actual model1 model2 model3 model4 baseline\n",
       "0    cat    cat    dog    cat    dog      dog\n",
       "1    dog    dog    cat    cat    dog      dog\n",
       "2    dog    cat    cat    cat    dog      dog\n",
       "3    dog    dog    dog    cat    dog      dog\n",
       "4    cat    cat    cat    dog    dog      dog"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test to make sure it worked\n",
    "dfg.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['actual', 'model1', 'model2', 'model3', 'model4', 'baseline']"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create list of column names\n",
    "cols = list(dfg.columns)\n",
    "cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) In terms of accuracy, how do the various models compare to the baseline model? Are any of the models better than the baseline?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline accuracy is 65.08%\n",
      "Model 1 accuracy is 80.74%\n",
      "Model 2 accuracy is 63.04%\n",
      "Model 3 accuracy is 50.96%\n",
      "Model 4 accuracy is 74.26%\n",
      "Model 1, is greater than the baseline. 80.74%\n",
      "Model 4, is greater than the baseline. 74.26%\n"
     ]
    }
   ],
   "source": [
    "# accuracy first, compare everything to the actual\n",
    "\n",
    "baseline_a = (dfg.actual == dfg.baseline).mean()\n",
    "model1_a = (dfg.actual == dfg.model1).mean()\n",
    "model2_a = (dfg.actual == dfg.model2).mean()\n",
    "model3_a = (dfg.actual == dfg.model3).mean()\n",
    "model4_a = (dfg.actual == dfg.model4).mean()\n",
    "\n",
    "print(f'Baseline accuracy is {baseline_a:.2%}')\n",
    "print(f'Model 1 accuracy is {model1_a:.2%}')\n",
    "print(f'Model 2 accuracy is {model2_a:.2%}')\n",
    "print(f'Model 3 accuracy is {model3_a:.2%}')\n",
    "print(f'Model 4 accuracy is {model4_a:.2%}')\n",
    "\n",
    "# create list of model accuracies \n",
    "accuracylist = [model1_a, model2_a, model3_a, model4_a]\n",
    "\n",
    "model = 0\n",
    "for x in accuracylist:\n",
    "    model = model + 1\n",
    "    if x > baseline_a:\n",
    "        print(f'Model {model}, is greater than the baseline. {x:.2%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a) Model 1 and 4 perform better than the baseline by a significant amount. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Suppose you are working on a team that solely deals with dog pictures. Which of these models would you recomend for Phase I? For Phase II?\n",
    "- for working with dog pictures\n",
    "- We will keep dog as the PAWsitive case \n",
    "- Accuracy is a good metric for the phase 1 - MODEL 1\n",
    "- Precision is a good measure for phase 2 - MODEL 2\n",
    "    - don't want cat pictures to slip through and make it to the website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline precision is 65.08%\n",
      "Model 1 precision is 89.00%\n",
      "Model 2 precision is 89.32%\n",
      "Model 3 precision is 65.99%\n",
      "Model 4 precision is 73.12%\n"
     ]
    }
   ],
   "source": [
    "# set positive case \n",
    "PAWsitive = 'dog'\n",
    "\n",
    "# create subset for every model where it equals the positive\n",
    "subset_b = dfg[dfg.baseline == PAWsitive]\n",
    "subset_1 = dfg[dfg.model1 == PAWsitive]\n",
    "subset_2 = dfg[dfg.model2 == PAWsitive]\n",
    "subset_3 = dfg[dfg.model3 == PAWsitive]\n",
    "subset_4 = dfg[dfg.model4 == PAWsitive]\n",
    "\n",
    "# calculate precision using each subset dataframe\n",
    "precisionb = (subset_b.actual == subset_b.baseline).mean()\n",
    "precision1 = (subset_1.actual == subset_1.model1).mean()\n",
    "precision2 = (subset_2.actual == subset_2.model2).mean()\n",
    "precision3 = (subset_3.actual == subset_3.model3).mean()\n",
    "precision4 = (subset_4.actual == subset_4.model4).mean()\n",
    "\n",
    "# print it all out\n",
    "print(f'Baseline precision is {precisionb:.2%}')\n",
    "print(f'Model 1 precision is {precision1:.2%}')\n",
    "print(f'Model 2 precision is {precision2:.2%}')\n",
    "print(f'Model 3 precision is {precision3:.2%}')\n",
    "print(f'Model 4 precision is {precision4:.2%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For Precision, Model 2 is best, followed closely by model 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline has a recall of 100.00%\n",
      "Model 1 has a recall of 80.33%\n",
      "Model 2 has a recall of 49.08%\n",
      "Model 3 has a recall of 50.86%\n",
      "Model 4 has a recall of 95.57%\n"
     ]
    }
   ],
   "source": [
    "# here I caluclated recall but ended up not needing it. But useful to have and see\n",
    "# set positive case\n",
    "positive = 'dog'\n",
    "\n",
    "# for recall care about the actuals so create a subset with the actual predictions of the positive\n",
    "subset = dfg[dfg.actual == positive]\n",
    "\n",
    "#calculate recall \n",
    "recallb = (subset.actual == subset.baseline).mean()\n",
    "recall1 = (subset.actual == subset.model1).mean()\n",
    "recall2 = (subset.actual == subset.model2).mean()\n",
    "recall3 = (subset.actual == subset.model3).mean()\n",
    "recall4 = (subset.actual == subset.model4).mean()\n",
    "\n",
    "# print it out \n",
    "print(f'Baseline has a recall of {recallb:.2%}')\n",
    "print(f'Model 1 has a recall of {recall1:.2%}')\n",
    "print(f'Model 2 has a recall of {recall2:.2%}')\n",
    "print(f'Model 3 has a recall of {recall3:.2%}')\n",
    "print(f'Model 4 has a recall of {recall4:.2%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) Suppose you are working on a team that solely deals with cat pictures. Which of these models would you recomend for Phase I? For Phase II?\n",
    "- I will try the same methods, and switch the positive case to cat\n",
    "- Accuracy for phase 1\n",
    "    - Also could be argued that Recall could be used as an effective metric for measuring mthe model \n",
    "    - because recall will capture all the possible cats out there. \n",
    "- Precision for phase 2 - MODEL 4 works best for cats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline precision is nan%\n",
      "Model 1 precision is 68.98%\n",
      "Model 2 precision is 48.41%\n",
      "Model 3 precision is 35.83%\n",
      "Model 4 precision is 80.72%\n"
     ]
    }
   ],
   "source": [
    "# set positive case \n",
    "positive = 'cat'\n",
    "\n",
    "# create subset for every model where it equals the positive\n",
    "subset_b = dfg[dfg.baseline == positive]\n",
    "subset_1 = dfg[dfg.model1 == positive]\n",
    "subset_2 = dfg[dfg.model2 == positive]\n",
    "subset_3 = dfg[dfg.model3 == positive]\n",
    "subset_4 = dfg[dfg.model4 == positive]\n",
    "\n",
    "# calculate precision using each subset dataframe\n",
    "precisionb = (subset_b.actual == subset_b.baseline).mean()\n",
    "precision1 = (subset_1.actual == subset_1.model1).mean()\n",
    "precision2 = (subset_2.actual == subset_2.model2).mean()\n",
    "precision3 = (subset_3.actual == subset_3.model3).mean()\n",
    "precision4 = (subset_4.actual == subset_4.model4).mean()\n",
    "\n",
    "# print it all out\n",
    "print(f'Baseline precision is {precisionb:.2%}')\n",
    "print(f'Model 1 precision is {precision1:.2%}')\n",
    "print(f'Model 2 precision is {precision2:.2%}')\n",
    "print(f'Model 3 precision is {precision3:.2%}')\n",
    "print(f'Model 4 precision is {precision4:.2%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5\n",
    "Follow the links below to read the documentation about each function, then apply those functions to the data from the previous problem.\n",
    "\n",
    "- [sklearn.metrics.accuracy_score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html)\n",
    "- [sklearn.metrics.precision_score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_score.html)\n",
    "- [sklearn.metrics.recall_score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.recall_score.html)\n",
    "- [sklearn.metrics.classification_report](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score for model 1 is 80.74%\n"
     ]
    }
   ],
   "source": [
    "# compute accuracy score for the dog cat question 4a\n",
    "\n",
    "accuracy_s_1 = accuracy_score(dfg.actual, dfg.model1)\n",
    "print(f'Accuracy score for model 1 is {accuracy_s_1:.2%}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for model1 is 80.74%\n",
      "Accuracy for model2 is 63.04%\n",
      "Accuracy for model3 is 50.96%\n",
      "Accuracy for model4 is 74.26%\n"
     ]
    }
   ],
   "source": [
    "# use a list \n",
    "model_list = ['model1', 'model2', 'model3', 'model4']\n",
    "\n",
    "for model in model_list:\n",
    "    accuracy = accuracy_score(dfg.actual, dfg[model])\n",
    "    print(f'Accuracy for {model} is {accuracy:.2%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision for model1, where dog is the Positive is 89.00%\n",
      "Precision for model2, where dog is the Positive is 89.32%\n",
      "Precision for model3, where dog is the Positive is 65.99%\n",
      "Precision for model4, where dog is the Positive is 73.12%\n"
     ]
    }
   ],
   "source": [
    "# calculate precision for question 4 a (this would be for Phase 2)\n",
    "# in precision_score you need to put pos_label = whatever your positive is\n",
    "for model in model_list:\n",
    "    precision = precision_score(dfg.actual, dfg[model], pos_label='dog')\n",
    "    print(f'Precision for {model}, where dog is the Positive is {precision:.2%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.803318992009834"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# practice calculating recall\n",
    "recall = recall_score(dfg.actual, dfg.model1, pos_label = 'dog')\n",
    "recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall for model1, where dog is positive is 80.33%\n",
      "Recall for model2, where dog is positive is 49.08%\n",
      "Recall for model3, where dog is positive is 50.86%\n",
      "Recall for model4, where dog is positive is 95.57%\n"
     ]
    }
   ],
   "source": [
    "# run all recalls with a for loop to print them all out\n",
    "for model in model_list:\n",
    "    recall = recall_score(dfg.actual, dfg[model], pos_label='dog')\n",
    "    print(f'Recall for {model}, where dog is positive is {recall:.2%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'              precision    recall  f1-score   support\\n\\n         cat       0.69      0.82      0.75      1746\\n         dog       0.89      0.80      0.84      3254\\n\\n    accuracy                           0.81      5000\\n   macro avg       0.79      0.81      0.80      5000\\nweighted avg       0.82      0.81      0.81      5000\\n'"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test out class report\n",
    "class_report = classification_report(dfg.actual, dfg.model1)\n",
    "# if you just call the variable you get a really ugly output\n",
    "class_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         cat       0.69      0.82      0.75      1746\n",
      "         dog       0.89      0.80      0.84      3254\n",
      "\n",
      "    accuracy                           0.81      5000\n",
      "   macro avg       0.79      0.81      0.80      5000\n",
      "weighted avg       0.82      0.81      0.81      5000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# use print to fix this\n",
    "print(class_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLASSIFICATION REPORT FOR MODEL1\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         cat       0.69      0.82      0.75      1746\n",
      "         dog       0.89      0.80      0.84      3254\n",
      "\n",
      "    accuracy                           0.81      5000\n",
      "   macro avg       0.79      0.81      0.80      5000\n",
      "weighted avg       0.82      0.81      0.81      5000\n",
      "\n",
      "------------------------------------------\n",
      "CLASSIFICATION REPORT FOR MODEL2\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         cat       0.48      0.89      0.63      1746\n",
      "         dog       0.89      0.49      0.63      3254\n",
      "\n",
      "    accuracy                           0.63      5000\n",
      "   macro avg       0.69      0.69      0.63      5000\n",
      "weighted avg       0.75      0.63      0.63      5000\n",
      "\n",
      "------------------------------------------\n",
      "CLASSIFICATION REPORT FOR MODEL3\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         cat       0.36      0.51      0.42      1746\n",
      "         dog       0.66      0.51      0.57      3254\n",
      "\n",
      "    accuracy                           0.51      5000\n",
      "   macro avg       0.51      0.51      0.50      5000\n",
      "weighted avg       0.55      0.51      0.52      5000\n",
      "\n",
      "------------------------------------------\n",
      "CLASSIFICATION REPORT FOR MODEL4\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         cat       0.81      0.35      0.48      1746\n",
      "         dog       0.73      0.96      0.83      3254\n",
      "\n",
      "    accuracy                           0.74      5000\n",
      "   macro avg       0.77      0.65      0.66      5000\n",
      "weighted avg       0.76      0.74      0.71      5000\n",
      "\n",
      "------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# do it in a loop! \n",
    "\n",
    "for model in model_list:\n",
    "    class_report = classification_report(dfg.actual, dfg[model])\n",
    "    print(f'CLASSIFICATION REPORT FOR {model.upper()}\\n')\n",
    "    print(class_report)\n",
    "    print('------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
